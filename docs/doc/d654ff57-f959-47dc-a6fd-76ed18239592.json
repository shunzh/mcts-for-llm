{
    "summary": "This code implements MCTS algorithm for a reinforcement learning agent in dyna-gym environment, initializing trees and updating value functions through backpropagation. The class includes attributes like rollouts, horizon, gamma, is_model_dynamic, default_policy, and lambda_coeff, with display method showing attribute values and act method using mcts_procedure to get optimal action from environment.",
    "details": [
        {
            "comment": "This code defines the MCTS (Monte Carlo Tree Search) algorithm for a reinforcement learning agent. The algorithm uses tree search procedures and chance nodes to explore and exploit the environment's state space. It also provides functions for determining chance node values and selecting tree policies, as well as handling termination conditions and modes for return calculation.",
            "location": "\"/media/root/Prima/works/mcts-for-llm/docs/src/dyna_gym/agents/mcts.py\":0-39",
            "content": "\"\"\"\nMCTS Algorithm\nRequired features of the environment:\nenv.state\nenv.action_space\nenv.transition(s ,a , is_model_dynamic)\nenv.equality_operator(s1, s2)\n\"\"\"\nimport random\nfrom gym import spaces\nfrom tqdm import tqdm\nfrom dyna_gym.utils.utils import combinations, multigpu_breakpoint\ndef chance_node_value(node, mode=\"best\"):\n    \"\"\"\n    Value of a chance node\n    \"\"\"\n    if len(node.sampled_returns) == 0:\n        return 0\n    elif mode == \"best\":\n        # max return (reasonable because the model is deterministic?)\n        return max(node.sampled_returns)\n    elif mode == \"sample\":\n        # Use average return\n        return sum(node.sampled_returns) / len(node.sampled_returns)\n    else:\n        raise Exception(f\"Unknown tree search mode {mode}\")\ndef mcts_tree_policy(children):\n    return random.choice(children)\ndef mcts_procedure(ag, tree_policy, env, done, root=None, term_cond=None, ts_mode=\"sample\"):\n    \"\"\"\n    Compute the entire MCTS procedure wrt to the selected tree policy.\n    Funciton tree_policy is a function taking an agent + a list of ChanceNodes as argument"
        },
        {
            "comment": "This function initializes a new tree if no existing one is provided, otherwise ensures the root is updated correctly. It then performs rollouts (a specified number based on agent configuration) while optionally terminating early if a provided condition is met. Rewards are collected along the tree for each rollout.",
            "location": "\"/media/root/Prima/works/mcts-for-llm/docs/src/dyna_gym/agents/mcts.py\":40-63",
            "content": "    and returning the one chosen by the tree policy.\n    Args:\n        ag: the agent\n        tree_policy: the action selection policy\n        env: the gym environment\n        done: whether the current state is terminal\n        root: the root of the tree, reuse the tree if not None, otherwise create a new tree\n        term_cond: termination condition, if not None, the procedure will terminate when term_cond() is True\n        ts_mode: the mode for tree search, can be 'sample', 'best'\n    \"\"\"\n    decision_node_num = 0\n    if root is not None:\n        # if using existing tree, making sure the root is updated correctly\n        assert root.state == env.state\n    else:\n        # create an empty tree\n        root = DecisionNode(None, env.state, ag.action_space.copy(), done, default_policy=ag.default_policy, id=decision_node_num)\n        decision_node_num += 1\n    for _ in tqdm(range(ag.rollouts), desc=\"Rolling out\", leave=False):\n        if term_cond is not None and term_cond():\n            break\n        rewards = [] # Rewards collected along the tree for the current rollout"
        },
        {
            "comment": "The code is traversing the tree using a selection process, moving from DecisionNode to ChanceNode. For each ChanceNode, it samples the next state s' and reward r(s,a,s') from the environment. If s' isn't already in the tree, a new DecisionNode is created; otherwise, the existing node is used. The code checks for terminal states and updates select accordingly. Rewards are appended to a list.",
            "location": "\"/media/root/Prima/works/mcts-for-llm/docs/src/dyna_gym/agents/mcts.py\":64-84",
            "content": "        node = root # Current node\n        terminal = done\n        # Selection\n        select = True\n        while select:\n            if type(node) == DecisionNode: # DecisionNode\n                if node.is_terminal:\n                    select = False # Selected a terminal DecisionNode\n                else:\n                    node = tree_policy(node.children) # Move down the tree, node is now a ChanceNode\n            else: # ChanceNode\n                # Given s, a, sample s' ~ p(s'|s,a), also get the reward r(s,a,s') and whether s' is terminal\n                state_p, reward, terminal = env.transition(node.parent.state, node.action, ag.is_model_dynamic)\n                rewards.append(reward)\n                new_state = True\n                # find if s' is already in the tree, if so point node to the corresponding DecisionNode (and new_state=False)\n                # if not, create a new DecisionNode for s' and point node to it\n                for i in range(len(node.children)):\n                    if env.equality_operator(node.children[i].state, state_p):"
        },
        {
            "comment": "This code snippet is part of the MCTS algorithm for an agent in a game environment. It handles node expansion and evaluation, updating the tree with new nodes as needed. If a new state (s') is encountered, it adds a new DecisionNode to the tree and updates the current node. Then it starts the evaluation process by traversing through the decision tree using the default policy or taking random actions depending on the game's configuration.",
            "location": "\"/media/root/Prima/works/mcts-for-llm/docs/src/dyna_gym/agents/mcts.py\":85-109",
            "content": "                        # s' is already in the tree\n                        node = node.children[i]\n                        new_state = False\n                        break\n                if new_state:\n                    # Selected a node for expansion\n                    select = False\n                    # Expansion to create a new DecisionNode\n                    new_node = DecisionNode(node, state_p, ag.action_space.copy(), terminal, default_policy=ag.default_policy, id=decision_node_num)\n                    node.children.append(new_node)\n                    decision_node_num += 1\n                    node = node.children[-1]\n        # Evaluation\n        # now `rewards` collected all rewards in the ChanceNodes above this node\n        assert(type(node) == DecisionNode)\n        state = node.state\n        current_state = state\n        if ag.default_policy is None:\n            t = 0\n            estimate = 0\n            while (not terminal) and (t < ag.horizon):\n                action = env.action_space.sample()"
        },
        {
            "comment": "This code performs Monte Carlo Tree Search (MCTS) in the dyna-gym environment. It calculates rewards and updates the value function for a given state and action based on whether the resulting state is terminal or not. If lambda_coeff is greater than zero, it adjusts the estimate with the value function.",
            "location": "\"/media/root/Prima/works/mcts-for-llm/docs/src/dyna_gym/agents/mcts.py\":110-132",
            "content": "                state, reward, terminal = env.transition(state, action, ag.is_model_dynamic)\n                estimate += reward * (ag.gamma**t)\n                t += 1\n        else:\n            if not node.is_terminal:\n                # follow the default policy to get a terminal state\n                state = ag.default_policy.get_predicted_sequence(state)\n                estimate = env.get_reward(state)\n                ag.rolled_out_trajectories.append(state[0])\n                ag.rolled_out_rewards.append(estimate)\n                # also save this to current nodes for possible visualization\n                node.info['complete_program'] = state[0]\n            else:\n                # the rewards are defined on terminating actions, the terminal states have no rewards\n                estimate = 0\n        if ag.lambda_coeff > 0:\n            assert ag.value_func is not None, \"value_func must be provided if lambda_coeff > 0\"\n            state_ids = current_state[0]\n            value = ag.value_func(state_ids)\n            estimate = ag.lambda_coeff * value + (1 - ag.lambda_coeff) * estimate"
        },
        {
            "comment": "The code is performing backpropagation of rewards in Monte Carlo Tree Search (MCTS) agent. It updates the visited count and stores sampled returns at each node, starting from the leaf node and moving up to the root node. The process ends when all rewards have been propagated. The code also includes an assert statement to ensure that there are no remaining rewards after backpropagation.",
            "location": "\"/media/root/Prima/works/mcts-for-llm/docs/src/dyna_gym/agents/mcts.py\":134-165",
            "content": "        # Backpropagation\n        node.visits += 1\n        node = node.parent\n        assert(type(node) == ChanceNode)\n        while node:\n            if len(rewards) != 0:\n                estimate = rewards.pop() + ag.gamma * estimate\n            node.sampled_returns.append(estimate)\n            node.parent.visits += 1\n            node = node.parent.parent\n        # should finish backpropagating all the rewards\n        assert len(rewards) == 0\n    return max(root.children, key=lambda n: chance_node_value(n, mode=ts_mode)).action, root\nclass DecisionNode:\n    \"\"\"\n    Decision node class, labelled by a state\n    Args:\n        default_policy: default policy, used to prioritize and filter possible actions\n    \"\"\"\n    def __init__(self, parent, state, possible_actions=[], is_terminal=False, default_policy=None, id=None):\n        self.id = id\n        self.parent = parent\n        self.state = state\n        self.is_terminal = is_terminal\n        if self.parent is None: # Root node\n            self.depth = 0\n        else: # Non root node"
        },
        {
            "comment": "This code creates a decision node in a Monte Carlo Tree Search (MCTS) algorithm. It sets the depth of the node, initializes action scores and possible actions based on default policy or uniform probability if no default policy is provided. The node populates its children with ChanceNodes using the possible actions and action scores. The node ensures it will be visited at least once for p-uct selection and initializes explored_children and visits variables.",
            "location": "\"/media/root/Prima/works/mcts-for-llm/docs/src/dyna_gym/agents/mcts.py\":166-188",
            "content": "            self.depth = parent.depth + 1\n        if default_policy is None:\n            self.possible_actions = possible_actions\n            random.shuffle(self.possible_actions)\n            # if no default policy is provided, assume selection probability is uniform\n            self.action_scores = [1.0 / len(self.possible_actions)] * len(self.possible_actions)\n        else:\n            # get possible actions from default policy\n            top_k_predict, top_k_scores = default_policy.get_top_k_tokens(self.state)\n            self.possible_actions = top_k_predict\n            self.action_scores = top_k_scores\n        # populate its children\n        self.children = [ChanceNode(self, (act, score))\n                         for act, score in zip(self.possible_actions, self.action_scores)]\n        self.explored_children = 0\n        # this decision node should be visited at least once, otherwise p-uct makes no sense for this node\n        self.visits = 1\n        # used to save any information of the state\n        # we use this for saving complete programs generated from it"
        },
        {
            "comment": "This code defines a class for the MCTS agent in a reinforcement learning context. It includes an initialization method that takes action space, number of rollouts, horizon, discount factor, is_model_dynamic flag, and default policy as parameters. The class also contains methods for selecting actions, updating node states, and expanding nodes based on the chosen action. The code defines ChanceNode and MCTS classes to represent nodes in the tree and the agent itself.",
            "location": "\"/media/root/Prima/works/mcts-for-llm/docs/src/dyna_gym/agents/mcts.py\":189-221",
            "content": "        self.info = {}\n    def is_fully_expanded(self):\n        return all([child.expanded() for child in self.children])\nclass ChanceNode:\n    \"\"\"\n    Chance node class, labelled by a state-action pair\n    The state is accessed via the parent attribute\n    \"\"\"\n    def __init__(self, parent, action_and_score):\n        self.parent = parent\n        self.action = action_and_score[0]\n        self.depth = parent.depth\n        self.children = []\n        self.prob = action_and_score[1] # the probability that this action should be token, provided by default policy\n        self.sampled_returns = []\n    def expanded(self):\n        return len(self.children) > 0\nclass MCTS(object):\n    \"\"\"\n    MCTS agent\n    \"\"\"\n    def __init__(self, action_space, rollouts=100, horizon=100, gamma=0.9, is_model_dynamic=True, default_policy=None):\n        if type(action_space) == spaces.discrete.Discrete:\n            self.action_space = list(combinations(action_space))\n        else:\n            self.action_space = action_space\n        self.n_actions = len(self.action_space)"
        },
        {
            "comment": "This code defines a class for an MCTS agent with attributes like rollouts, horizon, gamma, is_model_dynamic, default_policy, and lambda_coeff. The display method shows the values of these attributes. The act method uses the mcts_procedure to get the optimal action from the environment based on the current state and done condition.",
            "location": "\"/media/root/Prima/works/mcts-for-llm/docs/src/dyna_gym/agents/mcts.py\":222-243",
            "content": "        self.rollouts = rollouts\n        self.horizon = horizon\n        self.gamma = gamma\n        self.is_model_dynamic = is_model_dynamic\n        self.default_policy = default_policy\n        self.lambda_coeff = 0.0\n    def display(self):\n        \"\"\"\n        Display infos about the attributes.\n        \"\"\"\n        print('Displaying MCTS agent:')\n        print('Action space       :', self.action_space)\n        print('Number of actions  :', self.n_actions)\n        print('Rollouts           :', self.rollouts)\n        print('Horizon            :', self.horizon)\n        print('Gamma              :', self.gamma)\n        print('Is model dynamic   :', self.is_model_dynamic)\n    def act(self, env, done):\n        opt_act, _, = mcts_procedure(self, mcts_tree_policy, env, done)\n        return opt_act"
        }
    ]
}