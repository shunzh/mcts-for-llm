{
    "summary": "HuggingFaceDefaultPolicy uses a transformer model to make decisions in gym environments, employing generate_state and get_top_k_tokens functions for sequence prediction. The code calculates top k probabilities and indices, finding tokens with cumulative probability under 1.",
    "details": [
        {
            "comment": "HuggingFaceDefaultPolicy is a DefaultPolicy class that utilizes a HuggingFace transformer model for policy-based decision making. It takes in a gym environment, horizon, and pretrained model as inputs. get_predicted_sequence method generates the predicted sequence based on input data and attention mask with maximum length equal to either specified or default horizon.",
            "location": "\"/media/root/Prima/works/mcts-for-llm/docs/src/dyna_gym/default_policy/hf_default_policy.py\":0-37",
            "content": "from dyna_gym.default_policy.default_policy import DefaultPolicy\nimport gym\nimport torch\nfrom transformers import PreTrainedModel\nclass HuggingFaceDefaultPolicy(DefaultPolicy):\n    \"\"\"\n    Default policy that uses a HuggingFace transformer model.\n    \"\"\"\n    def __init__(\n            self,\n            env: gym.Env,\n            horizon: int,\n            model: PreTrainedModel,\n            generation_args: dict = {},\n    ):\n        super().__init__(env, horizon)\n        self.model = model\n        self.generate_args = generation_args\n    @torch.no_grad()\n    def get_predicted_sequence(self, state, horizon=None):\n        ids, attention_mask = state\n        horizon = horizon if horizon is not None else self.horizon\n        # Create a batch dimension\n        input_data = ids.unsqueeze(0)\n        attention_mask = attention_mask.unsqueeze(0)\n        outputs = self.model.generate(\n            inputs=input_data,\n            attention_mask=attention_mask,\n            max_length=horizon,\n            early_stopping=True,\n            return_dict_in_generate=True,"
        },
        {
            "comment": "This code defines two functions: \"generate_state\" and \"get_top_k_tokens\". The generate_state function takes inputs, passes them through a model, extracts the sequence and attention mask, and updates the attention mask. The get_top_k_tokens function takes a state, uses it to input into the model, obtains logits for tokens, converts logits to probabilities, and returns them.",
            "location": "\"/media/root/Prima/works/mcts-for-llm/docs/src/dyna_gym/default_policy/hf_default_policy.py\":38-71",
            "content": "            use_cache=True,\n            **self.generate_args\n        )\n        sequence = outputs.sequences.squeeze(0)\n        attention_mask = attention_mask.squeeze(0)\n        # update attention mask\n        num_new_tokens = sequence.shape[-1] - input_data.shape[-1]\n        attention_mask = torch.cat([attention_mask, torch.ones(num_new_tokens).to(attention_mask.device)])\n        return sequence, attention_mask\n    @torch.no_grad()\n    def get_top_k_tokens(self, state):\n        k = self.generate_args['top_k']\n        p = self.generate_args['top_p']\n        ids, attention_mask = state\n        # Create a batch dimension\n        input_data = ids.unsqueeze(0)\n        attention_mask = attention_mask.unsqueeze(0)\n        outputs = self.model(\n            input_ids=input_data,\n            attention_mask=attention_mask,\n        )\n        # Assuming the model returns logits for tokens\n        logits = outputs.logits[0][-1]  # First (and only) batch, last token\n        # Convert logits to probabilities\n        all_probs = torch.softmax(logits, dim=-1)"
        },
        {
            "comment": "This code calculates the top k probabilities and their indices for a given input, then computes the cumulative sum of sorted probabilities. It finds tokens where the cumulative sum exceeds a threshold p, and selects the smallest set of tokens whose cumulative probability is less than or equal to 1. The function returns final token indices and their probabilities.",
            "location": "\"/media/root/Prima/works/mcts-for-llm/docs/src/dyna_gym/default_policy/hf_default_policy.py\":73-89",
            "content": "        # Get the top k probabilities and their indices, already sorted\n        topk_probs, topk_indices = torch.topk(all_probs, k, sorted=True)\n        # Compute the cumulative sum of the sorted probabilities\n        cumsum_probs = torch.cumsum(topk_probs, dim=-1)\n        # Find tokens where the cumulative sum exceeds p\n        exceed_p_mask = cumsum_probs > p\n        # Find the smallest set of tokens whose cumulative probability exceeds p\n        mask = exceed_p_mask.cumsum(dim=-1) <= 1\n        # Apply the mask to get final tokens and their probabilities\n        final_indices = topk_indices[mask].tolist()\n        final_probs = topk_probs[mask].tolist()\n        return final_indices, final_probs"
        }
    ]
}